"""
long_term.py

Long Term Memory - ChromaDB Persistentë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ë©”ëª¨ë¦¬ ì €ì¥
"""

import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class LongTermMemory:
    """
    ChromaDB Persistentë¥¼ ì‚¬ìš©í•œ ì¥ê¸° ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
    
    ëŒ€í™” ë‚´ìš©ì„ ì„ë² ë”©í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì €ì¥
    """

    def __init__(
        self,
        persist_directory: str = "data/memory_db",
        collection_name: str = "conversation_memories"
    ):
        """
        ì¥ê¸° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”

        Args:
            persist_directory: ChromaDB ì €ì¥ ê²½ë¡œ
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name

        # OpenAI client for embeddings
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.embed_model = os.getenv("EMBED_MODEL", "text-embedding-3-small")

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Persistent)
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine", "description": "Long-term conversation memories"}
        )

        print(f"âœ… Long-term memory initialized at {persist_directory}")
        print(f"ğŸ“Š Collection '{collection_name}' has {self.collection.count()} memories")

    def save_memory(
        self,
        user_query: str,
        assistant_response: str,
        context: Optional[Dict[str, Any]] = None,
        importance: float = 0.5
    ) -> str:
        """
        ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ ì¥ê¸° ì €ì¥ì†Œì— ì €ì¥

        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            assistant_response: ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ë„êµ¬ ì‚¬ìš©, ê²€ìƒ‰ ê²°ê³¼ ë“±)
            importance: ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ (0.0 ~ 1.0)

        Returns:
            ì €ì¥ëœ ë©”ëª¨ë¦¬ì˜ ID
        """
        # ë©”ëª¨ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        memory_text = f"User: {user_query}\nAssistant: {assistant_response}"
        if context:
            context_str = json.dumps(context, ensure_ascii=False)
            memory_text += f"\nContext: {context_str}"

        # ì„ë² ë”© ìƒì„±
        response = self.openai_client.embeddings.create(
            model=self.embed_model,
            input=[memory_text]
        )
        embedding = response.data[0].embedding

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        timestamp = datetime.now().isoformat()
        # ì•ˆì „í•œ ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•´ì‹œ ì¡°í•©)
        hash_value = hashlib.md5(memory_text.encode()).hexdigest()[:8]
        timestamp_safe = timestamp.replace(":", "-").replace(".", "-")
        memory_id = f"memory_{timestamp_safe}_{hash_value}"

        metadata = {
            "user_query": user_query,
            "assistant_response": assistant_response,
            "timestamp": timestamp,
            "importance": importance,
            "context": json.dumps(context, ensure_ascii=False) if context else ""
        }

        # ChromaDBì— ì €ì¥
        self.collection.add(
            ids=[memory_id],
            documents=[memory_text],
            embeddings=[embedding],
            metadatas=[metadata]
        )

        print(f"ğŸ’¾ Saved memory: {memory_id[:20]}... (importance: {importance:.2f})")
        return memory_id

    def search_memories(
        self,
        query: str,
        top_k: int = 5,
        min_importance: float = 0.0
    ) -> List[Dict[str, Any]]:
        """
        ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ë©”ëª¨ë¦¬ ê°œìˆ˜
            min_importance: ìµœì†Œ ì¤‘ìš”ë„ í•„í„°

        Returns:
            ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        response = self.openai_client.embeddings.create(
            model=self.embed_model,
            input=[query]
        )
        query_embedding = response.data[0].embedding

        # ChromaDB ê²€ìƒ‰
        # ì¤‘ìš”ë„ í•„í„°ë§ì€ ê²€ìƒ‰ í›„ì— ì ìš© (ChromaDB where ì ˆ ì œí•œ)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k * 2 if min_importance > 0 else top_k  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°
        )

        # ê²°ê³¼ ì •ë¦¬ ë° ì¤‘ìš”ë„ í•„í„°ë§
        formatted_results = []

        if results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                importance = float(metadata.get("importance", 0.0))
                
                # ì¤‘ìš”ë„ í•„í„°ë§
                if importance < min_importance:
                    continue
                
                formatted_results.append({
                    "id": results['ids'][0][i],
                    "text": results['documents'][0][i],
                    "user_query": metadata.get("user_query", ""),
                    "assistant_response": metadata.get("assistant_response", ""),
                    "timestamp": metadata.get("timestamp", ""),
                    "importance": importance,
                    "context": json.loads(metadata.get("context", "{}")) if metadata.get("context") else {},
                    "distance": results['distances'][0][i] if results['distances'] else 0.0
                })
                
                # top_k ê°œìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
                if len(formatted_results) >= top_k:
                    break

        return formatted_results

    def get_recent_memories(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        ìµœê·¼ ë©”ëª¨ë¦¬ ì¡°íšŒ

        Args:
            limit: ë°˜í™˜í•  ë©”ëª¨ë¦¬ ê°œìˆ˜

        Returns:
            ìµœê·¼ ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        # ChromaDBì—ì„œ ëª¨ë“  ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬)
        all_results = self.collection.get()
        
        if not all_results['ids']:
            return []

        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì •ë¦¬
        memories = []
        for i, memory_id in enumerate(all_results['ids']):
            metadata = all_results['metadatas'][i] if all_results['metadatas'] else {}
            memories.append({
                "id": memory_id,
                "text": all_results['documents'][i],
                "user_query": metadata.get("user_query", ""),
                "assistant_response": metadata.get("assistant_response", ""),
                "timestamp": metadata.get("timestamp", ""),
                "importance": float(metadata.get("importance", 0.0)),
                "context": json.loads(metadata.get("context", "{}")) if metadata.get("context") else {}
            })

        # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        memories.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
        
        return memories[:limit]

    def count(self) -> int:
        """ì €ì¥ëœ ë©”ëª¨ë¦¬ ê°œìˆ˜"""
        return self.collection.count()

    def clear(self) -> None:
        """ëª¨ë“  ë©”ëª¨ë¦¬ ì‚­ì œ"""
        self.client.delete_collection(self.collection_name)
        self.collection = self.client.create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine", "description": "Long-term conversation memories"}
        )
        print(f"ğŸ—‘ï¸  All memories cleared")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_long_term_memory_instance: Optional[LongTermMemory] = None


def get_long_term_memory() -> LongTermMemory:
    """ì¥ê¸° ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    global _long_term_memory_instance
    if _long_term_memory_instance is None:
        _long_term_memory_instance = LongTermMemory()
    return _long_term_memory_instance

