"""
reflection.py

Reflection - ìë™ ë©”ëª¨ë¦¬ ì €ì¥ ë¡œì§
ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¥ê¸° ë©”ëª¨ë¦¬ì— ìë™ ì €ì¥
"""

import os
from typing import Dict, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

from .long_term import get_long_term_memory

load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")


def should_save_memory(state: Dict[str, Any]) -> bool:
    """
    ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
    
    Args:
        state: AgentState
        
    Returns:
        ì €ì¥ ì—¬ë¶€
    """
    # ìµœì¢… ë‹µë³€ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
    has_final_answer = bool(state.get("final_answer"))
    has_user_query = bool(state.get("user_query"))
    
    print(f"[Reflection] ë©”ëª¨ë¦¬ ì €ì¥ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸:")
    print(f"  - final_answer ì¡´ì¬: {has_final_answer}")
    print(f"  - user_query ì¡´ì¬: {has_user_query}")
    
    if not has_final_answer:
        print(f"[Reflection] âŒ ì €ì¥ ë¶ˆê°€: final_answer ì—†ìŒ")
        return False
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
    if not has_user_query:
        print(f"[Reflection] âŒ ì €ì¥ ë¶ˆê°€: user_query ì—†ìŒ")
        return False
    
    print(f"[Reflection] âœ… ì €ì¥ ê°€ëŠ¥")
    return True


def calculate_importance(user_query: str, assistant_response: str, context: Dict[str, Any]) -> float:
    """
    ë©”ëª¨ë¦¬ ì¤‘ìš”ë„ ê³„ì‚°
    
    Args:
        user_query: ì‚¬ìš©ì ì§ˆë¬¸
        assistant_response: ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ìš”ë„ (0.0 ~ 1.0)
    """
    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë„êµ¬ ì‚¬ìš© ì—¬ë¶€, ì‘ë‹µ ê¸¸ì´, íŠ¹ì • í‚¤ì›Œë“œ ë“±
    importance = 0.3  # ê¸°ë³¸ ì¤‘ìš”ë„
    print(f"[Reflection] ì¤‘ìš”ë„ ê³„ì‚° ì‹œì‘:")
    print(f"  - ê¸°ë³¸ ì¤‘ìš”ë„: {importance}")
    
    # ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì¤‘ìš”ë„ ì¦ê°€
    tool_used = context.get("tool_used", False)
    if tool_used:
        importance += 0.3
        print(f"  - ë„êµ¬ ì‚¬ìš© (+0.3): {importance}")
    
    # RAG ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ê²½ìš° ì¤‘ìš”ë„ ì¦ê°€
    rag_used = context.get("rag_used", False)
    if rag_used:
        importance += 0.2
        print(f"  - RAG ì‚¬ìš© (+0.2): {importance}")
    
    # ì‘ë‹µì´ ê¸´ ê²½ìš° (ìƒì„¸í•œ ì •ë³´ ì œê³µ)
    response_len = len(assistant_response)
    if response_len > 200:
        importance += 0.1
        print(f"  - ê¸´ ì‘ë‹µ (+0.1, {response_len}ì): {importance}")
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ë‚˜ ê°œì¸ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°
    preference_keywords = ["ì¢‹ì•„", "ì„ í˜¸", "ì‹«ì–´", "ê´€ì‹¬", "ì›í•´", "ì›í•˜ëŠ”", "ê¸°ì–µ"]
    has_preference = any(keyword in user_query for keyword in preference_keywords)
    if has_preference:
        importance += 0.2
        print(f"  - ì„ í˜¸ë„ í‚¤ì›Œë“œ (+0.2): {importance}")
    
    # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    final_importance = min(importance, 1.0)
    print(f"[Reflection] ìµœì¢… ì¤‘ìš”ë„: {final_importance:.2f}")
    return final_importance


def reflect_and_save(state: Dict[str, Any]) -> Optional[str]:
    """
    Reflection ë…¸ë“œ - ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥
    
    Args:
        state: AgentState
        
    Returns:
        ì €ì¥ëœ ë©”ëª¨ë¦¬ ID (ì €ì¥í•˜ì§€ ì•Šì€ ê²½ìš° None)
    """
    if not should_save_memory(state):
        return None
    
    user_query = state.get("user_query", "")
    assistant_response = state.get("final_answer", "")
    
    if not user_query or not assistant_response:
        return None
    
    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    context = {
        "tool_used": state.get("tool_result") is not None,
        "rag_used": len(state.get("retrieved_contexts", [])) > 0,
        "retrieved_contexts_count": len(state.get("retrieved_contexts", [])),
    }
    
    # ì¤‘ìš”ë„ ê³„ì‚°
    importance = calculate_importance(user_query, assistant_response, context)
    
    # ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì„ íƒì  ì €ì¥)
    if importance < 0.3:
        print(f"[Reflection] âŒ ì €ì¥ ìŠ¤í‚µ: ì¤‘ìš”ë„ {importance:.2f} < 0.3 (ìµœì†Œ ì„ê³„ê°’)")
        return None
    
    # ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥
    print(f"[Reflection] ğŸ’¾ ë©”ëª¨ë¦¬ ì €ì¥ ì‹œë„ (ì¤‘ìš”ë„: {importance:.2f})...")
    try:
        long_term_memory = get_long_term_memory()
        memory_id = long_term_memory.save_memory(
            user_query=user_query,
            assistant_response=assistant_response,
            context=context,
            importance=importance
        )
        print(f"[Reflection] âœ… ë©”ëª¨ë¦¬ ì €ì¥ ì„±ê³µ: {memory_id}")
        return memory_id
    except Exception as e:
        print(f"[Reflection] âš ï¸  ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_relevant_memories(query: str, top_k: int = 3) -> list:
    """
    í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê³¼ê±° ë©”ëª¨ë¦¬ ê²€ìƒ‰
    
    Args:
        query: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ë°˜í™˜í•  ë©”ëª¨ë¦¬ ê°œìˆ˜
        
    Returns:
        ê´€ë ¨ ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[Reflection] ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ìš”ì²­: '{query[:50]}...' (top_k={top_k})")
    try:
        long_term_memory = get_long_term_memory()
        memories = long_term_memory.search_memories(query, top_k=top_k)
        print(f"[Reflection] ê²€ìƒ‰ ê²°ê³¼: {len(memories)}ê°œ ë©”ëª¨ë¦¬ ë°œê²¬")
        return memories
    except Exception as e:
        print(f"[Reflection] âš ï¸  ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return []


def format_memories_for_context(memories: list) -> str:
    """
    ë©”ëª¨ë¦¬ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        memories: ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í¬ë§·ëœ ë©”ëª¨ë¦¬ ë¬¸ìì—´
    """
    if not memories:
        print(f"[Reflection] ë©”ëª¨ë¦¬ í¬ë§·íŒ…: ë©”ëª¨ë¦¬ ì—†ìŒ")
        return ""
    
    print(f"[Reflection] ë©”ëª¨ë¦¬ í¬ë§·íŒ…: {len(memories)}ê°œ ë©”ëª¨ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
    formatted = "\n\n[ê³¼ê±° ëŒ€í™” ê¸°ë¡]\n"
    for i, memory in enumerate(memories, 1):
        formatted += f"\n{i}. ì‚¬ìš©ì: {memory.get('user_query', '')}\n"
        formatted += f"   ì–´ì‹œìŠ¤í„´íŠ¸: {memory.get('assistant_response', '')[:200]}...\n"
    
    print(f"[Reflection] í¬ë§·ëœ ë©”ëª¨ë¦¬ ê¸¸ì´: {len(formatted)}ì")
    return formatted


